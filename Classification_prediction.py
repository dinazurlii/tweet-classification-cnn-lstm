# -*- coding: utf-8 -*-
"""Dina Crawl data twitter (ANALYSIS) > 2000 tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Bl10hcfW3tJlModzguIEl4tCYwbelKu
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
file_name = list(uploaded.keys())[0]

import pandas as pd

df = pd.read_csv('processed_labeled_tweets.csv')

df['joined_stemmed'] = df['tweet_stemmed'].apply(lambda x: ' '.join(eval(x)))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Encode label
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df['label'])

X = df['joined_stemmed'].values
y = df['label_encoded'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Tokenizer
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_len = max(len(seq) for seq in X_train_seq)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense

cnn_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=max_len),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
cnn_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)

from tensorflow.keras.layers import LSTM

lstm_model = Sequential([
    Embedding(input_dim=5000, output_dim=128, input_length=max_len),
    LSTM(64),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)

from sklearn.metrics import classification_report

y_pred_cnn = cnn_model.predict(X_test_pad).argmax(axis=1)
y_pred_lstm = lstm_model.predict(X_test_pad).argmax(axis=1)

print("=== CNN Classification Report ===")
print(classification_report(y_test, y_pred_cnn, target_names=le.classes_))

print("=== LSTM Classification Report ===")
print(classification_report(y_test, y_pred_lstm, target_names=le.classes_))

def clean_text(text):
    import re
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def predict_single(sentence, model):
    cleaned = clean_text(sentence.lower())
    seq = tokenizer.texts_to_sequences([cleaned])
    padded = pad_sequences(seq, maxlen=max_len, padding='post')
    pred = model.predict(padded)
    return le.classes_[pred.argmax()]

def predict_batch(sentences, model):
    cleaned = [clean_text(s.lower()) for s in sentences]
    seqs = tokenizer.texts_to_sequences(cleaned)
    padded = pad_sequences(seqs, maxlen=max_len, padding='post')
    preds = model.predict(padded)
    return [(s, le.classes_[p.argmax()]) for s, p in zip(sentences, preds)]

print("Prediksi 1 kalimat:")
print(predict_single("angin puting beliung parah melanda jakarta", cnn_model))

print("\nPrediksi 10 kalimat:")
kalimat_uji = [
    "angin puting beliung di cianjur", "pemilu 2024 sudah dekat", "banjir di semarang",
    "jokowi hadir di acara peresmian", "tsunami di jepang",
    "politik indonesia memanas", "longsor akibat hujan",
    "presiden prabowo pidato", "bencana banjir dan angin puting beliung di bogor",
    "netralitas kpu dipertanyakan"
]
for kalimat, label in predict_batch(kalimat_uji, cnn_model):
    print(f"{kalimat} â†’ {label}")