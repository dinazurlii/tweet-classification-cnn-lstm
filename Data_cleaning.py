# -*- coding: utf-8 -*-
"""Dina Crawl data twitter (CLEANING) > 2000 tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l38qEhprU_FOzGpJu796bcrO6hu1PkEk
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
file_name = list(uploaded.keys())[0]

df_raw = pd.read_csv(file_name, encoding='utf-8')

print("Kolom yang tersedia:", df_raw.columns.tolist())

df = df_raw[['full_text']].rename(columns={'full_text': 'tweet'})

print("Jumlah data:", len(df))
df.head()

df['tweet_clean'] = df['tweet'].str.lower()

import nltk
from nltk.tokenize import TweetTokenizer
nltk.download('punkt')

tokenizer = TweetTokenizer()

def clean_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

df['tweet_tokens'] = df['tweet_clean'].apply(lambda x: tokenizer.tokenize(clean_text(x)))

nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('indonesian'))
df['tweet_filtered'] = df['tweet_tokens'].apply(lambda x: [word for word in x if word not in stop_words])

normalisasi_kata = {
    'gk': 'tidak', 'ga': 'tidak', 'gak': 'tidak', 'tdk': 'tidak',
    'dr': 'dari', 'yg': 'yang', 'dgn': 'dengan'
}

def normalize(tokens):
    return [normalisasi_kata.get(token, token) for token in tokens]

df['tweet_normalized'] = df['tweet_filtered'].apply(normalize)

!pip install Sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

stemmer = StemmerFactory().create_stemmer()
df['tweet_stemmed'] = df['tweet_normalized'].apply(lambda x: [stemmer.stem(word) for word in x])

keyword_bencana = ['banjir', 'gempa', 'gunung', 'erupsi', 'tsunami', 'longsor', 'angin', 'puting beliung', 'hujan', 'bencana']
keyword_politik = ['jokowi', 'prabowo', 'pemilu', 'politik', 'kpu', 'presiden', 'gibran', 'pilpres', 'rakyat', 'pilkada', 'negara',
                   'ganjar', 'abah', 'ahok', 'ijazah']

def classify_label(tokens):
    if not isinstance(tokens, list):
        return 'lainnya'
    joined = ' '.join(tokens)
    if any(word in joined for word in keyword_bencana):
        return 'bencana'
    elif any(word in joined for word in keyword_politik):
        return 'politik'
    else:
        return 'lainnya'

df['label'] = df['tweet_stemmed'].apply(classify_label)

df['label'].value_counts()

df.to_csv('processed_labeled_tweets.csv', index=False)

from google.colab import files
files.download('processed_labeled_tweets.csv')